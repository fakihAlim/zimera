{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container Object\n",
    "Container Objects adalah obyek yang mempunyai sifat sebagai penampung, berisi lebih dari satu obyek.\n",
    "Source: https://spacy.io/api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container Objects 1 - Language\n",
    "Source: https://spacy.io/api/language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lang.en.English object at 0x000002D44EFCFB50>\n",
      "<spacy.strings.StringStore object at 0x000002D464AF2E00>\n",
      "83839\n",
      "Is 'speech' an English word?: True\n",
      "Is ' siapa ' an English word?: False\n",
      "0 Aznar\n",
      "1 Jonathan\n",
      "2 Hardest\n",
      "3 flawlessly\n",
      "4 Cocom\n",
      "5 Liberation\n",
      "6 Jonson\n",
      "7 frameworks\n",
      "8 ashes\n",
      "9 alisky\n"
     ]
    }
   ],
   "source": [
    "# Definisi language - menggunakan spacy.load\n",
    "import spacy\n",
    "import itertools\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# --- sampai di sini ----\n",
    "\n",
    "print(nlp)\n",
    "\n",
    "print(nlp.vocab.strings)\n",
    "\n",
    "print(len(nlp.vocab.strings))\n",
    "words = set(nlp.vocab.strings)\n",
    "word = 'speech'\n",
    "print(f\"Is '{word}' an English word?: {word in words}\")\n",
    "word = 'siapa'\n",
    "print(\"Is '\" ,word, \"' an English word?:\", word in words)\n",
    "\n",
    "for i, val in enumerate(itertools.islice(words, 10)):\n",
    "    print(i, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lang.en.English object at 0x000002D453907B20>\n",
      "<spacy.strings.StringStore object at 0x000002D4014B5090>\n"
     ]
    }
   ],
   "source": [
    "# Definisi language - cara lain\n",
    "import spacy\n",
    "\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "print(nlp)\n",
    "print(nlp.vocab.strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.language.Language object at 0x000002D44F0629A0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nPenting: kelas Language akan memproses teks dan menghasilkan obyek Doc. \\nDoc merupakan container objects. \\nKita akan mempelajari sedikit demi sedikit dari arsitektur tersebut, \\ndimulai dari container objects.\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jika ingin menggunakan API untuk language - vocab\n",
    "# tanpa definisi bahasa tertentu\n",
    "# (membangun dari awal)\n",
    "\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.language import Language\n",
    "nlp = Language(Vocab())\n",
    "print(nlp)\n",
    "\n",
    "'''\n",
    "Penting: kelas Language akan memproses teks dan menghasilkan obyek Doc. \n",
    "Doc merupakan container objects. \n",
    "Kita akan mempelajari sedikit demi sedikit dari arsitektur tersebut, \n",
    "dimulai dari container objects.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container Objects 2 - Doc\n",
    "Lengkap: https://spacy.io/api/doc\n",
    "\n",
    "Merupakan container untuk mengakses anotasi linguistik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nama\n",
      "saya\n",
      "joni\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"nama saya joni\")\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello 5\n",
      "world 5\n",
      "! 1\n",
      "hello world!\n"
     ]
    }
   ],
   "source": [
    "# Inisialisasi Doc - cara lain\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "words = [\"hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False] # artinya setelah hallo ada space (true), setelah world tidak ada space (False) dan setelah ! tidak ada space (False)\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "for token in doc:\n",
    "    print(token, len(token))\n",
    "    \n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "objects\n",
      "Doc is\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"A Doc is a sequence of Token objects\")\n",
    "\n",
    "print(doc[0].text)\n",
    "print(doc[-1].text)\n",
    "span = doc[1:3]\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Doc', 'is', 'a', 'sequence', 'of', 'Token', 'objects']\n"
     ]
    }
   ],
   "source": [
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "(None, None, <function <lambda> at 0x000002D45CE0A8B0>, None)\n"
     ]
    }
   ],
   "source": [
    "# set_extension\n",
    "\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "city_getter = lambda doc: any(city in doc.text for city in (\"New York\", \"Paris\", \"Berlin\"))\n",
    "Doc.set_extension(\"has_city\", getter=city_getter, force=True)\n",
    "doc = nlp(\"I like paris\")\n",
    "print(doc._.has_city)\n",
    "extension = Doc.get_extension(\"has_city\")\n",
    "print(extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York 7 15 GPE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I like New York\")\n",
    "span = doc.char_span(7, 15, label=\"GPE\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Strong PERSON\n"
     ]
    }
   ],
   "source": [
    "# set entity and its label\n",
    "from spacy.tokens import Span\n",
    "\n",
    "doc = nlp(\"Mr. Strong flew to New York on Saturday morning, He go with Mr. Smith\")\n",
    "\n",
    "doc.set_ents([Span(doc, 0, 2, \"PERSON\")])\n",
    "ents = list(doc.ents)\n",
    "\n",
    "for word in ents:\n",
    "    print(word.text, word.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity dari apples - oranges 0.8885888838864333\n",
      "similarity dari oranges - apples 0.8885888838864333\n"
     ]
    }
   ],
   "source": [
    "# Doc similarity\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "apples = nlp(\"i like apples\")\n",
    "oranges = nlp(\"i love oranges\")\n",
    "\n",
    "apples_oranges = apples.similarity(oranges)\n",
    "oranges_apples = oranges.similarity(apples)\n",
    "\n",
    "print(\"similarity dari apples - oranges\",apples_oranges)\n",
    "print(\"similarity dari oranges - apples\",oranges_apples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container Objects 3 - DocBin\n",
    "Source: https://spacy.io/api/docbin\n",
    "\n",
    "Digunakan untuk packing obyek Doc untuk serialisasi biner. Serialisasi yang digunakan adalah msgpack yang dikompres menggunakan gzip dengan format: \n",
    "\n",
    "'''\n",
    "MSGPACK OBJECT STRUCTURE\n",
    "{\n",
    "    \"version\": str,           # DocBin version number\n",
    "    \"attrs\": List[uint64],    # e.g. [TAG, HEAD, ENT_IOB, ENT_TYPE]\n",
    "    \"tokens\": bytes,          # Serialized numpy uint64 array with the token data\n",
    "    \"spaces\": bytes,          # Serialized numpy boolean array with spaces data\n",
    "    \"lengths\": bytes,         # Serialized numpy int32 array with the doc lengths\n",
    "    \"strings\": List[str]      # List of unique strings in the token data\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "DocBin\n",
      "class\n",
      "lets\n",
      "you\n",
      "efficiently\n",
      "serialize\n",
      "the\n",
      "information\n",
      "from\n",
      "a\n",
      "collection\n",
      "of\n",
      "Doc\n",
      "objects\n",
      ".\n",
      "The\n",
      "DocBin\n",
      "class\n",
      "lets\n",
      "you\n",
      "efficiently\n",
      "serialize\n",
      "the\n",
      "information\n",
      "from\n",
      "a\n",
      "collection\n",
      "of\n",
      "Doc\n",
      "objects\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "doc_bin = DocBin(attrs=[\"LEMMA\"])\n",
    "doc = nlp(\"The DocBin class lets you efficiently serialize the information from a collection of Doc objects.\")\n",
    "\n",
    "doc_bin.add(doc)\n",
    "for token in doc:\n",
    "    print(token)  \n",
    "doc_bin.to_disk(\"./data.spacy\")\n",
    "\n",
    "doc_bin = DocBin().from_disk(\"./data.spacy\")\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container Objects 4 - Lexeme\n",
    "\n",
    "Source Lengkap: https://spacy.io/api/lexeme\n",
    "\n",
    "Suatu entry di vocabulary tanpa konteks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 4690420944186131903 X I I True False True en\n",
      "love 3702023516439754181 xxxx l ove True False False en\n",
      "coffee 3197928453018144401 xxxx c fee True False False en\n",
      "for 16037325823156266367 xxx f for True False False en\n",
      "$ 11283501755624150392 $ $ $ False False False en\n",
      "5 2090661578966068036 d 5 5 False True False en\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I love coffee for $5\")\n",
    "for word in doc:\n",
    "    lexeme = doc.vocab[word.text]\n",
    "    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n",
    "            lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_)\n",
    "    \n",
    "# orth => orthographic features, representasi text dalam hash\n",
    "# shape_ => bentuk teks, karakter => X atau x, hanya 4 awal. Jika numerik: d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container Objects Lain\n",
    "Masih ada beberapa container objects:\n",
    "\n",
    "Example: merupakan koleksi dari anotasi pelatihan. https://spacy.io/api/example\n",
    "Span: potongan dari obyek Doc. https://spacy.io/api/span\n",
    "SpanGroup: sekumpulan dari span, bisa diberi nama: https://spacy.io/api/spangroup\n",
    "Token: https://spacy.io/api/token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### contoh lain DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dasar\n",
      "contoh\n",
      "atas\n",
      "tadi\n",
      "maka\n",
      "coba\n",
      "guna\n",
      "sastrawi\n",
      "untuk\n",
      "rubah\n",
      "jadi\n",
      "kata\n",
      "dasar\n",
      "dalam\n",
      "bahasa\n",
      "indonesia\n",
      "kemudian\n",
      "simpan\n",
      "dalam\n",
      "docbin\n",
      "dasar\n",
      "contoh\n",
      "atas\n",
      "tadi\n",
      "maka\n",
      "coba\n",
      "guna\n",
      "sastrawi\n",
      "untuk\n",
      "rubah\n",
      "jadi\n",
      "kata\n",
      "dasar\n",
      "dalam\n",
      "bahasa\n",
      "indonesia\n",
      "kemudian\n",
      "simpan\n",
      "dalam\n",
      "docbin\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# doc_bin = DocBin(attrs=[\"LEMMA\"]) untuk lemma nya menggunakan sastrawi\n",
    "doc = nlp(stemmer.stem(\"Berdasarkan contoh diatas tadi, maka dicobalah menggunakan sastrawi untuk merubah menjadi kata dasar dalam bahasa indonesia kemudian disimpan dalam DocBin\"))\n",
    "\n",
    "doc_bin.add(doc)\n",
    "for token in doc:\n",
    "    print(token)  \n",
    "doc_bin.to_disk(\"./data_baru.spacy\")\n",
    "\n",
    "doc_bin = DocBin().from_disk(\"./data_baru.spacy\")\n",
    "for token in doc:\n",
    "    print(token)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1dbe5302cb3bb4c0fe4e492535fb181d3f49e6b02a7d389fb2104bcbd574eaf4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('py39-nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
