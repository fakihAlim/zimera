{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning dan spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "18\n",
      "['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "ner_lst = nlp.pipe_labels['ner']\n",
    "\n",
    "print(len(ner_lst))\n",
    "print(ner_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoenix GPE\n",
      "Arecaceae PERSON\n",
      "Africa LOC\n",
      "the Middle East LOC\n",
      "South Asia LOC\n",
      "Phoenix GPE\n",
      "12–19 CARDINAL\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "article = \"\"\"It’s that time of the year when Open Web builders \n",
    "head to the Web3 castle in Denver, Colorado. ETHDenver is upon us, \n",
    "and the NEAR community will be there with a full slate of events \n",
    "(February 15-20) focused on building a multi-chain future. \n",
    "\n",
    "Why is the NEAR community attending ETHDenver? Good question. \n",
    "The NEAR community believes in a collaborative, decentralized, \n",
    "multi-chain ecosystem of Open Web and Metaverse platforms. Projects \n",
    "like Aurora and Rainbow Bridge, for example, have helped build \n",
    "simple and secure bridges between NEAR and Ethererum, allowing \n",
    "users and developers to freely move assets between the two networks.\"\"\"\n",
    "\n",
    "doc=nlp(article)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.label_)\n",
    "\n",
    "\n",
    "#reverensi NER: https://nlp.cs.nyu.edu/ene/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result BEFORE training:\n",
      "     NONE\n",
      "Proses training ...\n",
      "Result AFTER training:\n",
      "     Festy DISTRICT\n"
     ]
    }
   ],
   "source": [
    "# Sumber: https://stackoverflow.com/questions/69181078/spacy-how-do-you-add-custom-ner-labels-to-a-pre-trained-model\n",
    "# dengan modifikasi\n",
    "#\n",
    "import spacy\n",
    "import random\n",
    "from spacy import util\n",
    "from spacy.tokens import Doc\n",
    "from spacy.training import Example\n",
    "from spacy.language import Language\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def print_doc_entities(_doc: Doc):\n",
    "    if _doc.ents:\n",
    "        for _ent in _doc.ents:\n",
    "            print(f\"     {_ent.text} {_ent.label_}\")\n",
    "    else:\n",
    "        print(\"     NONE\")\n",
    "\n",
    "train_data = [\n",
    "    ('We need to deliver it to Festy.', [(25, 30, 'DISTRICT')]),\n",
    "    ('', [])\n",
    "]\n",
    "\n",
    "# Result before training\n",
    "print(f\"\\nResult BEFORE training:\")\n",
    "# doc = nlp(u'I need a taxi to Festy.')\n",
    "doc = nlp(u'I like dates')\n",
    "print_doc_entities(doc)\n",
    "\n",
    "# Karena training untuk ner, maka komponen \n",
    "# ner saja yang akan diaktifkan\n",
    "#\n",
    "disabled_pipes = []\n",
    "for pipe_name in nlp.pipe_names:\n",
    "    if pipe_name != 'ner':\n",
    "        nlp.disable_pipes(pipe_name)\n",
    "        disabled_pipes.append(pipe_name)\n",
    "\n",
    "print(\"Proses training ...\")\n",
    "\n",
    "optimizer = nlp.create_optimizer()\n",
    "for _ in range(25):\n",
    "    random.shuffle(train_data)\n",
    "    for raw_text, entity_offsets in train_data:\n",
    "        doc = nlp.make_doc(raw_text)\n",
    "        example = Example.from_dict(doc, {\"entities\": entity_offsets})\n",
    "        nlp.update([example], sgd=optimizer)\n",
    "\n",
    "# Enable all previously disabled pipe components\n",
    "for pipe_name in disabled_pipes:\n",
    "    nlp.enable_pipe(pipe_name)\n",
    "\n",
    "# Result after training\n",
    "print(f\"Result AFTER training:\")\n",
    "doc = nlp(u'I need a taxi to Festy.')\n",
    "print_doc_entities(doc)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "output_dir = Path(os.path.join(current_dir, 'zimera.model/'))\n",
    "nlp.to_disk(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "akan ada folder model bernama zimera.model\n",
    "'''bash\n",
    "yrich@dll:~/Documents/GitHub/zimera/novice/03-09/zimera.model$ ls -la\n",
    "total 136\n",
    "drwxrwxr-x 10 yrich yrich  4096 Feb 14 20:14 .\n",
    "drwxrwxr-x  3 yrich yrich  4096 Feb 14 20:14 ..\n",
    "drwxrwxr-x  2 yrich yrich  4096 Feb 14 20:14 attribute_ruler\n",
    "-rw-rw-r--  1 yrich yrich  5400 Feb 14 20:14 config.cfg\n",
    "drwxrwxr-x  3 yrich yrich  4096 Feb 14 20:14 lemmatizer\n",
    "-rw-rw-r--  1 yrich yrich 10184 Feb 14 20:14 meta.json\n",
    "drwxrwxr-x  2 yrich yrich  4096 Feb 14 20:14 ner\n",
    "drwxrwxr-x  2 yrich yrich  4096 Feb 14 20:14 parser\n",
    "drwxrwxr-x  2 yrich yrich  4096 Feb 14 20:14 senter\n",
    "drwxrwxr-x  2 yrich yrich  4096 Feb 14 20:14 tagger\n",
    "drwxrwxr-x  2 yrich yrich  4096 Feb 14 20:14 tok2vec\n",
    "-rw-rw-r--  1 yrich yrich 77777 Feb 14 20:14 tokenizer\n",
    "drwxrwxr-x  2 yrich yrich  4096 Feb 14 20:14 vocab\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Festy', 'DISTRICT')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "model_dir = Path(os.path.join(current_dir, 'zimera.model/'))\n",
    "\n",
    "nlp=spacy.load(model_dir)\n",
    "\n",
    "doc = nlp(u'I need a taxi to Festy.')\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.id.examples import sentences\n",
    "\n",
    "doc = nlp.pipe(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b03c103e43e670ae080ce36f4c3aadf0b7d2fc5c8142e5a07a36765ec3b5d6c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
